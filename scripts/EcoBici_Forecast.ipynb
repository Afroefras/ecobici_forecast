{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/efraflores/Desktop/EF/Contests/Ecobici'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Control de datos\n",
    "from time import sleep\n",
    "from pathlib import Path, PosixPath\n",
    "from pickle import dump as save_pkl\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Ingeniería de variables\n",
    "from re import search, findall\n",
    "from numpy import array, nan\n",
    "from datetime import datetime\n",
    "from pandas import DataFrame, Series, read_csv, to_datetime, options, date_range, cut\n",
    "options.mode.chained_assignment = None\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Modelos\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gráficas\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "\n",
    "class BaseClass:\n",
    "    def __init__(self, base_dir: str, file_name: str='EcoBici') -> None:\n",
    "        '''\n",
    "        Inicializa la clase recibiendo un directorio y opcionalmente un nombre base\n",
    "        '''\n",
    "        # Obtiene un directorio como texto y convertirlo a tipo Path para unir directorios, buscar archivos, etc.\n",
    "        self.base_dir = Path(base_dir)\n",
    "        # Enlista todos los archivos con formato YYYY_MM en el directorio\n",
    "        self.files_list = [x for x in self.base_dir.glob('*.csv') if search(r'[\\/\\\\]\\d{4}\\-\\d{2}\\.csv',str(x))!=None]\n",
    "        # Nombre base para los archivos que vayan a exportarse\n",
    "        self.file_name = file_name\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        '''\n",
    "        Cantidad de archivos de interés en el directorio\n",
    "        '''\n",
    "        return len(self.files_list)\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'{self.__len__()} archivos en:\\n{self.base_dir}'\n",
    "\n",
    "\n",
    "    def cool_print(self, text: str, sleep_time: float=0.02, by_word: bool=False) -> None: \n",
    "        '''\n",
    "        Imprimir como si se fuera escribiendo\n",
    "        '''\n",
    "        acum = ''\n",
    "        for x in (text.split() if by_word else text): \n",
    "            # Acumular texto\n",
    "            acum += x+' ' if by_word else x\n",
    "            # Limpiar pantalla\n",
    "            clear_output(wait=True)\n",
    "            # Esperar un poco para emular efecto de escritura\n",
    "            sleep(sleep_time*(9 if by_word else 1))\n",
    "            # Imprimir texto acumulado\n",
    "            print(acum)\n",
    "        # Mantener el texto en pantalla\n",
    "        sleep(1.7)\n",
    "\n",
    "\n",
    "    def get_csv(self, file_path: PosixPath, just_name: bool=False, **kwargs) -> DataFrame: \n",
    "        '''\n",
    "        Obtener tabla a partir de un archivo .csv\n",
    "        '''\n",
    "        # Obtiene el nombre del archivo a partir de un directorio\n",
    "        file_name = str(file_path).split('/')[-1]\n",
    "        try: \n",
    "            # Si el parámetro lo indica, obtiene el csv conectando el directorio con el nombre del archivo\n",
    "            if just_name: df = read_csv(self.base_dir.joinpath(file_name), low_memory=False, **kwargs)\n",
    "            # De otro modo, utiliza el directorio que recibe como parámetro\n",
    "            else: df = read_csv(file_path, low_memory=False, **kwargs)\n",
    "            # Obtiene e informa del número de renglones y columnas\n",
    "            df_shape = df.shape\n",
    "            self.cool_print(f'Archivo con nombre {file_name} fue encontrado en:\\n{self.base_dir}\\nCon {df_shape[0]} renglones y {df_shape[-1]} columnas')\n",
    "            return df\n",
    "        # Imprime que hubo un error al obtener el csv\n",
    "        except: self.cool_print(f'No se encontró el archivo con nombre {file_name} en:\\n{self.base_dir}\\nSi el archivo csv existe, seguramente tiene un encoding y/o separador diferente a \"utf-8\" y \",\" respectivamente\\nIntenta de nuevo!')\n",
    "    \n",
    "\n",
    "    def export_csv(self, df: DataFrame, name_suffix=None, **kwargs) -> None: \n",
    "        '''\n",
    "        Exportar un archivo en formato csv\n",
    "        '''\n",
    "        # Define si habrá un sufijo para el nombre del archivo\n",
    "        export_name = f'{self.file_name}.csv' if name_suffix==None else f'{self.file_name}_{name_suffix}.csv'\n",
    "        # Exporta el archivo en el directorio base\n",
    "        df.to_csv(self.base_dir.joinpath(export_name), **kwargs)\n",
    "        # Informa al usuario\n",
    "        self.cool_print(f'Archivo: {export_name} fue exportado exitosamente en:\\n{self.base_dir}')\n",
    "\n",
    "\n",
    "    def rem_nan_rows(self, df: DataFrame, thres: float=1.0) -> DataFrame:\n",
    "        '''\n",
    "        Omitir registros mayor o igual al porcentaje \"thres\" de valores nulos\n",
    "        '''\n",
    "        to_remove = []\n",
    "        # enumerate(['A','B','C']) == zip(range(len(['A','B','C']),['A','B','C'])) == [(0,'A'), (1,'B'), (2,'C')]\n",
    "        for i,_ in enumerate(df.index):\n",
    "            # Revisar por renglón, transponiéndolo\n",
    "            sub_df = df.iloc[i,:].T\n",
    "            # Obtener el porcentaje de nulos\n",
    "            perc_nan = sub_df.isnull().mean()\n",
    "            # Si dicho porcentaje es mayor, guardar el lugar del renglón en una lista\n",
    "            if perc_nan >= thres: to_remove.append(i)\n",
    "\n",
    "        # Omitir los registros de la lista con el porcentaje de valores nulos más grande que el parámetro \"thres\"\n",
    "        df = df.loc[~df.index.isin(to_remove),:]\n",
    "        # Informar cuántos renglones fueron omitidos\n",
    "        self.cool_print(f'{len(to_remove)} renglones con {\"{:.1%}\".format(thres)}% o más de valores nulos fueron eliminados')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def create_bins(self, df: DataFrame, col: str, bins: list, lower_limit=-1, upper_limit=1000) -> Series:\n",
    "        '''\n",
    "        Recibiendo los cortes, recibe una columna numérica y crea rangos tipo \"00\", \"01 a 05\", \">=6\"\n",
    "        '''\n",
    "        # Función para convertir float: 1.0 --> str: '01'\n",
    "        def two_char(n): return str(int(n)).zfill(2)\n",
    "\n",
    "        # Crear rangos\n",
    "        df[f'{col}_range'] = cut(df[col], bins=[lower_limit]+bins+[upper_limit])\n",
    "        # Convertirlo a texto: [1.0 - 5.0] --> '01 a 05'\n",
    "        df[f'{col}_range'] = df[f'{col}_range'].map(lambda x: two_char(x.left+1)+' a '+two_char(x.right) if x!=nan else nan)\n",
    "\n",
    "        # Corregir algunas etiquetas como: '01 a 01'-->'01' y también '03 a upper_limit'-->'>= 03'\n",
    "        last_cut = two_char(bins[-1]+1)\n",
    "        df[[f'{col}_range']] = df[[f'{col}_range']].replace({\n",
    "            **{last_cut+f' a {upper_limit}': '>= '+last_cut},\n",
    "            **{two_char(x)+' a '+two_char(x): two_char(x) for x in bins}\n",
    "        })\n",
    "        # No perder de vista los valores ausentes: \"La falta de información también es información\"\n",
    "        df[f'{col}_range'] = df[f'{col}_range'].map(lambda x: nan if str(x)=='nan' else str(x))\n",
    "\n",
    "        return df[f'{col}_range']\n",
    "\n",
    "\n",
    "    def date_vars(self, df: DataFrame, date_col: str='fecha', hours_bin: list=[9,12,14,17,20], **kwargs) -> DataFrame: \n",
    "        '''\n",
    "        Crear variables de fecha: año, trimestre, mes, hora y rangos de hora\n",
    "        '''\n",
    "        # Convertir a tipo datetime\n",
    "        df[date_col] = to_datetime(df[date_col], **kwargs)\n",
    "\n",
    "        # Para extraer la división de año\n",
    "        df[f'{date_col}_year'] = df[date_col].dt.year.map(int).map(str)\n",
    "        # Trimestre a dos caracteres\n",
    "        df[f'{date_col}_quarter'] = df[date_col].dt.quarter.map(lambda x: str(int(x)).zfill(2))\n",
    "        # Mes a dos caracteres\n",
    "        df[f'{date_col}_month'] = df[date_col].dt.month.map(lambda x: str(int(x)).zfill(2))\n",
    "\n",
    "        # Concatenar el año, tanto trimestre como con el mes\n",
    "        df[f'{date_col}_yearquarter'] = df[f'{date_col}_year']+' - '+df[f'{date_col}_quarter']\n",
    "        df[f'{date_col}_yearmonth'] = df[f'{date_col}_year']+' - '+df[f'{date_col}_month']\n",
    "\n",
    "        # Día de la semana, sólo los primeros 3 caracteres\n",
    "        df[f'{date_col}_month'] = df[date_col].dt.day_name().str[:3]\n",
    "\n",
    "        # Hora\n",
    "        df[f'{date_col}_hour'] = df[date_col].dt.hour\n",
    "        # Crear rangos de hora\n",
    "        df[f'{date_col}_hour_range'] = self.create_bins(df, f'{date_col}_hour', bins=hours_bin)\n",
    "\n",
    "        # Mantener sólo la fecha\n",
    "        df[date_col] = df[date_col].dt.date\n",
    "        return df\n",
    "\n",
    "\n",
    "    def outliers(self, df: DataFrame, cols: list, rem_perc: float=0.03, rem_outliers: bool=True) -> DataFrame:\n",
    "        ''''\n",
    "        Mediante el modelo de sklearn, elimina los outliers analizando los datos de forma multivariada\n",
    "        '''\n",
    "        # Instancia el modelo con el % que reciba como parámetro\n",
    "        outlier = IsolationForest(contamination=rem_perc, n_jobs=-1)\n",
    "\n",
    "        # Indica con \"-1\" si el registro es atípico\n",
    "        df['outlier'] = outlier.fit_predict(df[cols])\n",
    "\n",
    "        # Omite dichos registros y la columna que indica si es atípico\n",
    "        if rem_outliers: df = df[df['outlier']!=-1].drop(columns = 'outlier')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def multishift(self, df: DataFrame, id_cols: list, date_col: str='fecha', shifts: list=range(1,22), rem_sum_zero: bool=True, to_pivot: bool=True, **pivot_args): \n",
    "        '''\n",
    "        Escalona los valores para crear una Tabla Analítica de Datos con formato: valor hoy, valor 1 día antes, dos días antes, etc\n",
    "        '''\n",
    "        # Asegurarse que tiene solamente la fecha\n",
    "        df[date_col] = df[date_col].map(to_datetime).dt.date\n",
    "\n",
    "        # Sólo una columna que servirá como ID\n",
    "        id_col = ','.join(id_cols)\n",
    "        df[id_col] = df[id_cols].astype(str).apply(','.join, axis=1)\n",
    "\n",
    "        # Omitir aquellos IDs con menor frequencia que el máximo valor de \"shifts\", porque inevitablemente tendrán shift vacíos\n",
    "        freq = df[id_col].value_counts().to_frame()\n",
    "        omit_idx = freq[freq[id_col]<=max(shifts)].index.to_list()\n",
    "        if len(omit_idx)>0: \n",
    "            df = df[~df[id_col].isin(omit_idx)].copy()\n",
    "        \n",
    "        # Columna auxiliar para conteo de registros\n",
    "        df['n'] = 1\n",
    "\n",
    "        if to_pivot:\n",
    "            # Estructurar una tabla pivote, de donde se partirá para \"recorrer\" los días\n",
    "            df = df.pivot_table(index=[id_col,date_col], **pivot_args, fill_value=0)\n",
    "            # Unir las posibles multi-columnas en una\n",
    "            df.columns = ['_'.join([x for x in col]) if not isinstance(df.columns[0],str) else col for col in df.columns]\n",
    "            df = df.reset_index()\n",
    "        \n",
    "        total = DataFrame()\n",
    "        for row in set(df[id_col]): \n",
    "            # Para cada grupo de renglones por ID\n",
    "            df_id = df.set_index(id_col).loc[row,: ]\n",
    "            # Asegurar todas las fechas\n",
    "            tot_dates = DataFrame(date_range(start=df_id[date_col].min(), end=df_id[date_col].max()).date, columns=[date_col])\n",
    "            df_id = df_id.merge(tot_dates, on=date_col, how='right').fillna(0)\n",
    "            cols = df_id.columns[1: ]\n",
    "\n",
    "            # Comenzar el \"escalonado\" de la tabla pivote inicial\n",
    "            aux = df_id.copy()\n",
    "            for i in shifts:\n",
    "                if i > 0:\n",
    "                    # Renombrar la columna que se acaba de escalonar\n",
    "                    aux = aux.join(df_id.iloc[:,1:].shift(i).rename(columns={x: f'{x}_{str(i).zfill(2)}' for x in cols}))\n",
    "            # No perder de vista el \"id\" de este subconjunto\n",
    "            aux[id_col] = row\n",
    "            # Agregar a la tabla total\n",
    "            total = total.append(aux, ignore_index=True)\n",
    "\n",
    "        # Mantener como índice para tener una matriz X de valores continuos\n",
    "        total.set_index(id_cols+[date_col], inplace=True)\n",
    "\n",
    "        # Omitir registros que suman 0?\n",
    "        if rem_sum_zero:\n",
    "            total['sum'] = total.sum(axis=1)\n",
    "            total = total[total['sum']>0].drop('sum', axis=1)\n",
    "\n",
    "        return total\n",
    "\n",
    "    \n",
    "    def apply_multishift(self, df: DataFrame, export_shifted: bool=True, **kwargs) -> tuple: \n",
    "        '''\n",
    "        Aplica el método anterior y devuelve la matriz de valores continuos no nulos: X y el vector resultado: y\n",
    "        '''\n",
    "        # Aplicar la función \"multishift\" con los parámetros personalizados\n",
    "        df = self.multishift(df.reset_index(), **kwargs)\n",
    "        df.dropna(inplace=True)\n",
    "        df = df[sorted(df.columns)].copy()\n",
    "\n",
    "        # Obtener la lista de las columnas de todos los días previos\n",
    "        prev = df.head(1).filter(regex='_\\d+').columns.tolist()\n",
    "        # Y aquellas originales, sin escalonar\n",
    "        actual = [x for x in df.columns if x not in prev]\n",
    "\n",
    "        # Ordenar las columnas\n",
    "        df = df[actual+prev].copy()\n",
    "        # Tal vez el usuario quiere exportar los resultados\n",
    "        if export_shifted: self.export_csv(df, name_suffix='shifted')\n",
    "\n",
    "        # Seleccionar los datos para construir f(X)=y\n",
    "        X = df[prev].copy()\n",
    "        y = df[actual].sum(axis=1).values\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def train_model(self, X: DataFrame, y: array, scaler=RobustScaler, model=LinearRegression, **kwargs): \n",
    "        '''\n",
    "        Escala y entrena un modelo, devuelve el score, el objeto tipo Pipeline y la relevancia de cada variable\n",
    "        '''\n",
    "        # Conjunto de entrenamiento y de test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=7, shuffle=True)\n",
    "\n",
    "        # Define los pasos del flujo\n",
    "        pipe_obj = Pipeline(steps=[('prep', scaler()), ('model', model(**kwargs))])\n",
    "\n",
    "        # Entrena y guarda el score en test\n",
    "        test_score = pipe_obj.fit(X_train,y_train).score(X_test, y_test)\n",
    "        # Guarda el score en train, para revisar sobreajuste\n",
    "        train_score = pipe_obj.score(X_train,y_train)\n",
    "\n",
    "        # Imprime los scores\n",
    "        self.cool_print(f\"Score: {'{:.2%}'.format(test_score)}\\nTraining score: {'{:.2%}'.format(train_score)}\")\n",
    "\n",
    "        # Elige la forma de obtener las variables más representativas\n",
    "        # Ya sea por Regresión Lineal\n",
    "        try: most_important_features = pipe_obj[-1].coef_ \n",
    "        except: \n",
    "            # O por Árbol de decisión, Bosque Aleatorio, XGBoost\n",
    "            try: most_important_features = pipe_obj[-1].feature_importances_\n",
    "            # De otro modo, solamente asignar un vector de 0s a este objeto\n",
    "            except: most_important_features = [0]*len(X.columns)\n",
    "\n",
    "        # Las ordena descendentemente\n",
    "        coef_var = DataFrame(zip(X.columns, most_important_features)).sort_values(1, ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Devuelve el objeto para clustering, la lista de scores tanto en train como en test y la relevancia de cada variable para el modelo \n",
    "        return pipe_obj, (test_score,train_score), coef_var\n",
    "\n",
    "\n",
    "    def real_vs_est(self, X: DataFrame, y: array, model, omit_zero: bool=True) -> DataFrame:\n",
    "        '''\n",
    "        Devuelve una tabla con dos columnas: el valor real y el valor predicho por el modelo\n",
    "        '''\n",
    "        # De todo el conjunto de datos...\n",
    "        df = X.join(DataFrame(y, index=X.index, columns=['real']))\n",
    "        # Predice el el valor...\n",
    "        df['estimado'] = model.predict(X)\n",
    "\n",
    "        # Si el parámetro lo indica, reemplaza negativos por 0\n",
    "        if omit_zero: df['estimado'] = df['estimado'].map(lambda x: max(0,x))\n",
    "\n",
    "        # Y devuelve sólo las columna real y la estimada\n",
    "        return df[['real','estimado']]\n",
    "\n",
    "    \n",
    "    def plot_real_vs_est(self, X: DataFrame, y: array, model, id_col: str, date_col: str, from_year: int=1900, to_year: int=datetime.now().year):\n",
    "        '''\n",
    "        Grafica la tendencia real y la predicha por el modelo a través del tiempo\n",
    "        ''' \n",
    "        # Obtener real vs estimado\n",
    "        pred = self.real_vs_est(X, y, model).reset_index()\n",
    "\n",
    "        # Filtrar sólo años de interés\n",
    "        pred['year'] = to_datetime(pred[date_col]).dt.year\n",
    "        df = pred[(pred['year']>=from_year)&(pred['year']<=to_year)].copy()\n",
    "        df.drop(columns='year', inplace=True)\n",
    "\n",
    "        # Mostrar comportamiento real vs estimado\n",
    "        df.set_index(id_col, inplace=True)\n",
    "        for x in set(df.index): \n",
    "            df_id = df.loc[x,: ].reset_index(drop=True).set_index(date_col)\n",
    "            df_id.iplot(title=x)\n",
    "\n",
    "\n",
    "    def save_model(self, model, name: str) -> None:\n",
    "        '''\n",
    "        Exporta el modelo en modo diccionario para que cuando se importe, se conozca de qué trata el objeto\n",
    "        '''\n",
    "        # Guarda el pickle con extensión \".xz\" para comprimirlo\n",
    "        with open(self.base_dir.joinpath(f'{name}.xz'), 'wb') as f:\n",
    "            # Como diccionario para conocer su nombre\n",
    "            save_pkl({name:model}, f)\n",
    "            \n",
    "        # Confirma que el archivo fue guardado exitosamente\n",
    "        self.cool_print(f'El modelo {name}.xz fue guardado existosamente en:\\n{self.base_dir}')\n",
    "\n",
    "\n",
    "class EcoBici(BaseClass):\n",
    "    def __init__(self, base_dir: str, file_name: str = 'EcoBici') -> None:\n",
    "        '''\n",
    "        Hereda los atributos de la clase base\n",
    "        '''\n",
    "        super().__init__(base_dir, file_name=file_name)\n",
    "\n",
    "\n",
    "    def get_by_hour_range(self, file_path: PosixPath, date_col: str='Fecha_Retiro', hour_col: str='Hora_Retiro', date_format: str=r'%d/%m/%Y', hour_format: str=r'%H:%M:%S', **kwargs) -> DataFrame:\n",
    "        '''\n",
    "        Importa un csv, lo limpia y transforma según los parámetros de \"pandas.pivot_table()\" que reciba\n",
    "        '''\n",
    "        # Lee una tabla en formato \".csv\"\n",
    "        df = self.get_csv(file_path)\n",
    "\n",
    "        # Omite los registros completamente nulos\n",
    "        df = self.rem_nan_rows(df)\n",
    "        # Omite los registros nulos en los campos de fecha u hora\n",
    "        df.dropna(subset=[date_col,hour_col], inplace=True)\n",
    "\n",
    "        # Une las columnas de fecha y hora\n",
    "        df['fecha'] = df[[date_col,hour_col]].apply(' '.join, axis=1)\n",
    "        # Aplica el método de crear variablesd de fecha y rangos de hora\n",
    "        df = self.date_vars(df, dayfirst=True, format=f'{date_format} {hour_format}')\n",
    "\n",
    "        # Crea una columna con el valor 1 en todos los registros y estructura la tabla como lo indiquen los parámetros\n",
    "        df = df.assign(n=1).pivot_table(**kwargs)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def read_raw_files(self, from_year: int=2000, to_year: int=datetime.now().year, export_result: bool=True, **kwargs) -> DataFrame:\n",
    "        ''''\n",
    "        Obtiene todos los archivos de los años indicados para reestructurarlos según los parámetros de \"pandas.pivot_table()\" que reciba\n",
    "        '''\n",
    "        # Filtra los archivos que cumplan con la condición: de X año a Y año\n",
    "        filtered_files = sorted([x for x in self.files_list if from_year <= int(findall(r'[\\/\\\\](\\d{4})\\-', str(x))[0]) <= to_year])\n",
    "\n",
    "        # DataFrame vacío para ir acumulando los datos\n",
    "        df = DataFrame()\n",
    "        for chunk_file in filtered_files:\n",
    "            # Aplicar el método anterior para agrupar por archivo y no los datos completos\n",
    "            transformed = self.get_by_hour_range(chunk_file, **kwargs)\n",
    "            # Acumular la tabla anterior con el archivo actual\n",
    "            df = df.append(transformed)\n",
    "            # Eliminar objeto para optimizar memoria\n",
    "            del transformed\n",
    "\n",
    "        # Volver a agrupar si es que el índice se repite en dos archivos\n",
    "        df = df.reset_index().pivot_table(index=df.index.names, values=df.columns, aggfunc=sum)\n",
    "\n",
    "        # Tal vez el usuario quiera exportar el resultado\n",
    "        if export_result: self.export_csv(df, name_suffix=f'from_{from_year}_to_{to_year}')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def ecobici_shifted(self, source, **kwargs) -> tuple:\n",
    "        '''\n",
    "        Aplica el método multishift para poder entrenar un modelo de regresión\n",
    "        '''\n",
    "        # Verifica si el parámetro es una tabla\n",
    "        if isinstance(source, DataFrame): df = source.copy()\n",
    "        # De otro modo, es el nombre del archivo ubicado en el directorio base\n",
    "        else: df = self.get_csv(source, just_name=True)\n",
    "\n",
    "        X, y = self.apply_multishift(df, **kwargs)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "eb = EcoBici(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>fecha_hour_range</th>\n",
       "      <th>Ciclo_Estacion_Retiro</th>\n",
       "      <th>fecha</th>\n",
       "      <th>00 a 09</th>\n",
       "      <th>10 a 12</th>\n",
       "      <th>13 a 14</th>\n",
       "      <th>15 a 17</th>\n",
       "      <th>18 a 20</th>\n",
       "      <th>&gt;= 21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-06-02</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-06-03</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-06-04</td>\n",
       "      <td>84</td>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "      <td>44</td>\n",
       "      <td>95</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>85</td>\n",
       "      <td>25</td>\n",
       "      <td>38</td>\n",
       "      <td>50</td>\n",
       "      <td>90</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "fecha_hour_range  Ciclo_Estacion_Retiro       fecha  00 a 09  10 a 12  \\\n",
       "0                                   1.0  2018-06-01       78       36   \n",
       "1                                   1.0  2018-06-02       25       26   \n",
       "2                                   1.0  2018-06-03       18       33   \n",
       "3                                   1.0  2018-06-04       84       27   \n",
       "4                                   1.0  2018-06-05       85       25   \n",
       "\n",
       "fecha_hour_range  13 a 14  15 a 17  18 a 20  >= 21  \n",
       "0                      31       60       62     19  \n",
       "1                      22       12       18     11  \n",
       "2                      16       24       16      2  \n",
       "3                      37       44       95     18  \n",
       "4                      38       50       90     16  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['Ciclo_Estacion_Retiro', 'fecha'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j4/1p33_bh96yn8pdz3b_5t80hm0000gn/T/ipykernel_4739/833190591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EcoBici_from_2018_to_2021.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mecobici_shifted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_pivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ciclo_Estacion_Retiro'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshifts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m360\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m372\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j4/1p33_bh96yn8pdz3b_5t80hm0000gn/T/ipykernel_4739/3194335094.py\u001b[0m in \u001b[0;36mecobici_shifted\u001b[0;34m(self, source, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjust_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_multishift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j4/1p33_bh96yn8pdz3b_5t80hm0000gn/T/ipykernel_4739/3194335094.py\u001b[0m in \u001b[0;36mapply_multishift\u001b[0;34m(self, df, export_shifted, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         '''\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Aplicar la función \"multishift\" con los parámetros personalizados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultishift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j4/1p33_bh96yn8pdz3b_5t80hm0000gn/T/ipykernel_4739/3194335094.py\u001b[0m in \u001b[0;36mmultishift\u001b[0;34m(self, df, id_cols, date_col, shifts, rem_sum_zero, to_pivot, **pivot_args)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Mantener como índice para tener una matriz X de valores continuos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mtotal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_cols\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# Omitir registros que suman 0?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/hub/ecobici_forecast/venv/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/hub/ecobici_forecast/venv/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5451\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5453\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['Ciclo_Estacion_Retiro', 'fecha'] are in the columns\""
     ]
    }
   ],
   "source": [
    "file_name = 'EcoBici_from_2018_to_2021.csv'\n",
    "\n",
    "aux = eb.ecobici_shifted(df.head(100), to_pivot=False, id_cols=['Ciclo_Estacion_Retiro'], shifts=list(range(22))+list(range(360,372)))\n",
    "aux[0].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: EcoBici_from_2018_to_2021.csv fue exportado exitosamente en:\n",
      "/Users/efraflores/Desktop/EF/Contests/Ecobici\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_hour_range</th>\n",
       "      <th>00 a 09</th>\n",
       "      <th>10 a 12</th>\n",
       "      <th>13 a 14</th>\n",
       "      <th>15 a 17</th>\n",
       "      <th>18 a 20</th>\n",
       "      <th>&gt;= 21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ciclo_Estacion_Retiro</th>\n",
       "      <th>fecha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>247.0</th>\n",
       "      <th>2021-08-29</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "fecha_hour_range                  00 a 09  10 a 12  13 a 14  15 a 17  18 a 20  \\\n",
       "Ciclo_Estacion_Retiro fecha                                                     \n",
       "247.0                 2021-08-29        5        4        0        2        2   \n",
       "\n",
       "fecha_hour_range                  >= 21  \n",
       "Ciclo_Estacion_Retiro fecha              \n",
       "247.0                 2021-08-29      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = eb.read_raw_files(from_year=2018, to_year=2021, index=['Ciclo_Estacion_Retiro','fecha'], columns='fecha_hour_range', values='n', aggfunc='count', fill_value=0)\n",
    "df.sample()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eef6e2c8b097c86032189ae171566cd5eb8f0439004e1de8adfca7562ca93029"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
